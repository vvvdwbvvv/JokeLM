#!/usr/bin/env python3
# save as post_train_pipeline.py
import argparse, os, json, torch, getpass
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

<<<<<<< HEAD

'''
python post_train_pipeline.py \
  --base_model Qwen/Qwen3-4B \
  --lora_dir ./output/qwen3-pun-lora \
  --merged_dir ./qwen3-pun-merged

# â· åŠ ä¸Š 4-bit é‡åŒ–
python post_train_pipeline.py \
  --base_model Qwen/Qwen3-4B \
  --lora_dir ./output/qwen3-pun-lora \
  --quant \
  --quant_dir ./qwen3-pun-merged-4bit

# â¸ ç›´æ¥æ¨ä¸Š Hugging Faceï¼ˆéœ€å…ˆè¨­å®š HF_TOKENï¼‰
export HF_TOKEN=hf_xxx
python post_train_pipeline.py \
  --base_model Qwen/Qwen3-4B \
  --lora_dir ./output/qwen3-pun-lora \
  --merged_dir ./qwen3-pun-merged \
  --push \
  --repo_id your_username/qwen3-4b-pun-tw
'''


=======
# ---------- CLI åƒæ•¸ ----------
>>>>>>> origin/main
def get_args():
    p = argparse.ArgumentParser()
    p.add_argument("--base_model", required=True,
                   help="åŸå§‹ï¼ˆæœªå¾®èª¿ï¼‰Model ID æˆ–æœ¬åœ°è·¯å¾‘")
    p.add_argument("--lora_dir",  required=True,
                   help="LoRA checkpoint ç›®éŒ„ (trainer.save_model çš„çµæœ)")
    p.add_argument("--merged_dir", default="./merged_model",
                   help="è¼¸å‡ºï¼šåˆä½µå¾Œæ¨¡å‹å„²å­˜è·¯å¾‘")
    p.add_argument("--quant_dir",  default="./merged_model_4bit",
                   help="è¼¸å‡ºï¼š4-bit é‡åŒ–æ¨¡å‹å„²å­˜è·¯å¾‘")
    p.add_argument("--quant", action="store_true",
                   help="å•Ÿç”¨ 4-bit bnb é‡åŒ–")
    p.add_argument("--push",  action="store_true",
                   help="æ¨é€è‡³ HuggingFace Hub")
    p.add_argument("--repo_id",
                   help="Hub Repositoryï¼Œä¾‹å¦‚ username/qwen3-pun-4b")
    p.add_argument("--sample_file",
                   help="JSON linesï¼Œæ¯è¡Œä¸€å€‹ promptï¼›è‹¥ç„¡å‰‡ç”¨å…§å»ºæ¨£ä¾‹")
    p.add_argument("--max_new_tokens", type=int, default=128)
    return p.parse_args()

<<<<<<< HEAD
=======
# ---------- å·¥å…·å‡½æ•¸ ----------
>>>>>>> origin/main
def load_prompts(file_path:str|None):
    if file_path and Path(file_path).is_file():
        return [json.loads(l)["prompt"] if l.strip().startswith("{") else l.strip()
                for l in Path(file_path).read_text().splitlines() if l.strip()]
<<<<<<< HEAD
=======
    # é è¨­æ¸¬è©¦é›†
>>>>>>> origin/main
    return [
        "èªªä¸€å€‹è·Ÿè²“æœ‰é—œçš„ç¬‘è©±",
        "èªªä¸€å€‹è·Ÿç¶ è±†æœ‰é—œçš„ç¬‘è©±ã€‚",
        "è«‹è¬›ä¸€å‰‡è·Ÿã€Œå·¥ç¨‹å¸«ã€ç›¸é—œçš„å†·ç¬‘è©±ã€‚"
    ]

def quick_test(model, tok, prompts, device, max_new):
    model.eval()
    for i, prompt in enumerate(prompts, 1):
        print(f"\n=== [æ¨£ä¾‹ {i}] {prompt}")
        inp = tok(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            out = model.generate(**inp,
                                 max_new_tokens=max_new,
                                 temperature=0.8,
                                 top_p=0.9)
        print(tok.decode(out[0], skip_special_tokens=True))

def bnb_quantize(src_dir:str, tgt_dir:str):
    cfg = BitsAndBytesConfig(load_in_4bit=True,
                             bnb_4bit_compute_dtype=torch.float16,
                             bnb_4bit_use_double_quant=True,
                             bnb_4bit_quant_type="nf4")
    model = AutoModelForCausalLM.from_pretrained(
        src_dir,
        quantization_config=cfg,
        device_map="auto"
    )
    model.save_pretrained(tgt_dir)
    print(f"âœ… 4-bit é‡åŒ–æ¨¡å‹å·²å­˜è‡³ {tgt_dir}")

# ---------- ä¸»æµç¨‹ ----------
def main():
    args = get_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"

    print("ğŸšš 1. è®€å– base model èˆ‡ LoRA...")
    base = AutoModelForCausalLM.from_pretrained(
        args.base_model, device_map="auto")
    tok  = AutoTokenizer.from_pretrained(
        args.base_model, trust_remote_code=True)
    model = PeftModel.from_pretrained(base, args.lora_dir)
    model.to(device)

    print("ğŸ”— 2. åˆä½µ LoRA æ¬Šé‡ (merge_and_unload)...")
    merged = model.merge_and_unload()
    Path(args.merged_dir).mkdir(parents=True, exist_ok=True)
    merged.save_pretrained(args.merged_dir)
    tok.save_pretrained(args.merged_dir)
    print(f"âœ… åˆä½µå¾Œæ¨¡å‹å­˜è‡³ {args.merged_dir}")

    print("ğŸ§ª 3. å¿«é€Ÿæ¨ç†æ¸¬è©¦")
    prompts = load_prompts(args.sample_file)
    quick_test(merged, tok, prompts, device, args.max_new_tokens)

    if args.quant:
        print("ğŸ”§ 4. é€²è¡Œ 4-bit bnb é‡åŒ–")
        bnb_quantize(args.merged_dir, args.quant_dir)

    if args.push:
        if not args.repo_id:
            raise ValueError("--push éœ€è¦æŒ‡å®š --repo_id")
        from huggingface_hub import login
        token = os.getenv("HF_TOKEN") or getpass.getpass("HF token â‰« ")
        login(token=token)
        print(f"â˜ï¸ 5. ä¸Šå‚³è‡³ Hubï¼š{args.repo_id}")
        merged.push_to_hub(args.repo_id)
        tok.push_to_hub(args.repo_id)
        if args.quant:
            from huggingface_hub import upload_folder
            upload_folder(repo_id=args.repo_id,
                          folder_path=args.quant_dir,
                          commit_message="Add 4-bit quantized version",
                          path_in_repo="4bit")
        print("âœ… æ¨é€å®Œæˆ")

if __name__ == "__main__":
    main()
